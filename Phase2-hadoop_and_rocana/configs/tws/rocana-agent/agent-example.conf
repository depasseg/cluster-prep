{
  kafka: {
    // brokers - Comma separated list of Kafka broker hosts where events should be published.
    //   This is a required setting, and if it is missing or invalid the agent will fail
    //   to start.
    // brokers: "kafkahost01.mydomain.com:9092,kafkahost02.mydomain.com:9092"

    batching: {
      // batch-size - The number of records to hold on to before performing a transmission to Kafka.
      //   Increasing this number will improve throughput as aproximately one RPC call to
      //   Kafka is performed per batch.  Note that increased batch size also risks an
      //   increased volume of data loss in the event of an unclean shutdown.
      //   Defaults to '500'.
      // batch-size: 500

      // timeout-ms - The number of milliseconds after the last batch flush when we should perform
      //   another flush even if we have not read 'BatchSize' number of records.
      //   Defaults to 1000 milliseconds.
      // timeout-ms: 1000

    }

    // events-topic - The name of the Kafka topic to which Events will be written.
    //   Defaults to 'events'.
    // events-topic: "events"

    // client-id - A user-specified string sent to Kafka with each request to help trace calls.
    //   This is an advanced configuration option that should only need to be changed
    //   to aid in debugging.
    // client-id: "rocana-agent"

    // number-workers - The number of workers launched to transmit message batches to Kafka. This
    //   translates to the maximum number of batches that can be in flight at any given point in time.
    //   Note that this only affects non-durable messages such as Syslog data. Replay-able data sources
    //   such as tailed file have their own batching settings. Defaults to '50'.
    // number-workers: 50

    // partitioner - Method for partitioning messages amongst sub-topics. Defaults to 'Random'.
    //   Accepted Values:
    //     Random - Chooses a random partition each time.
    //     RoundRobin - Walks through the available partitions one at a time.
    //     Hash - If the key is nil, or fails to encode, then a random partition is
    //       chosen. Otherwise the FNV-1a hash of the encoded bytes is used modulus the
    //       number of partitions. This ensures that messages with the same key always
    //       end up on the same partition.
    // partitioner: "Random"

    // required-acks - The level of acknowledgement you wish to receive before the message is to be
    //   considered delivered. There is a performance/reliability trade-off inherent in
    //   choosing a value for this setting.  Defaults to 'WaitForLocal'.
    //   Accepted Values:
    //     NoResponse - The producer never waits for an acknowledgement from the broker.
    //       This option provides the lowest latency but the weakest durability
    //       guarantees (some data will be lost when a server fails).
    //     WaitForLocal - The producer gets an acknowledgement after the leader replica
    //       has received the data. This option provides better durability as the client
    //       waits until the server acknowledges the request as successful (only
    //       messages that were written to the now-dead leader but not yet replicated
    //       will be lost).
    //     WaitForAll - The producer gets an acknowledgement after all in-sync replicas
    //       have received the data. This option provides the best durability, we
    //       guarantee that no messages will be lost as long as at least one in sync
    //       replica remains.
    // required-acks: "WaitForLocal"

    retries: {
      // number-retries - The number of attempts to make for any given message to be successfully sent
      //   to Kafka.  After this many attempts the message will be discarded and a warning
      //   will be issued. The exception to this is messages sourced from durable data
      //   sources (eg - tailing/spooling). These messages will be retried until either
      //   they are either successfully or the agent is shutdown. Defaults to '4'.
      // number-retries: 4

      // wait-ms - The number of milliseconds to wait before retrying to send messages to Kafka
      //   after they have failed to send on their first attempt.  For any subsequent
      //   attempts, the amount of time we wait will grow exponentially.
      //   Defaults to 250 milliseconds.
      // wait-ms: 250

    }
  }

  checkpoint-db: {
    // data-location - The directory where the agent will store checkpoint information about files
    //   that are being tailed and spooled. After processing a message from a file,
    //   the agent will write the byte offset where the message ended. In the event
    //   of a shutdown this offset will be read in and used to determine where to
    //   continue reading the file from.  The user running the agent will need write
    //   permission to this directory.
    // data-location: "${ROCANA_HOME}/state/rocana-agent/checkpoint-db"

  }

  // The 'primary-field-defaults' section allows configuration of the values for
  // important top level attributes in Events generated by the Agent.  These include
  // the Location, Host, and Service fields. Note that these defaults can be optionally
  // overridden for each data source to handle ingestion of data multiple services
  // running on the same machine. Additionally, data sources have the ability to be
  // configured to extract these values from the individual messages they process.
  primary-field-defaults: {
    // host - The host on which this agent runs. If no value is specified
    //   for this setting, it will default to the hostname according to the OS.
    // host: "host02.mydomain.com"

    // location - The location name where this agent runs. Typically, this is a data center,
    //   region, or other similar designation. You are free to encode hierarchical
    //   information in this value by separating components with slashes. Defaults to
    //   an empty string.
    //   Example: nyc-1/zone-2/rack-12
    // location: ""

    // service - The name of the service producing event data. Like 'host', this will be
    //   automatically discovered in most cases. When receiving events via syslog, the
    //   service will be taken from the service field of the syslog message.
    //   Example: rocana-agent
    // service: ""

  }

  logging: {
    file: {
      // enabled - Whether or not to log to disk
      //   Defaults to 'true'
      // enabled: true

      // level - The level of log messages written to disk. Messages below the specified level will be
      //   filtered. In order of highest to lowest priority: ERROR, WARN, INFO, DEBUG, or TRACE.
      //   Defaults to 'INFO'.
      // level: "INFO"

      // name - The filename to write log messages. Supports environment variable expansion.
      //   Defaults to "${ROCANA_HOME}/logs/rocana-agent.log"
      // name: "${ROCANA_HOME}/logs/rocana-agent.log"

      // max-files - The maximum number of rotated log files to keep. Set to 0 to keep all files.
      //   Defaults to 0
      // max-files: 0

      // compress - Whether or not to compress files after rotation.
      //   Defaults to 'false'.
      // compress: false

    }
  }

  // The sources section designates all event collection performed by the Agent, including
  // syslog data remote Linux hosts and devices, as well as application logs and Event Log
  // data from the Agentâ€™s own host. Every source requires an input type, which is demarcated
  // by a freely named object nested inside the sole sources: { } parent section of the Agent
  // configuration file. Source subsections can be named freely to help track what each Agent
  // is handling.
  sources: {

// Start sample syslog collection
    sample-syslog: {
      input: {
        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'.
        type: "syslog"

        tcp: {
          // enabled - Whether or not the agent should listen for Syslog events over TCP.
          //   Defaults to 'false'.
          // enabled: false

          // address - Address that the agent should listen on for TCP based syslog messages.
          //   Defaults to '0.0.0.0'.
          // address: "0.0.0.0"

          // port - Port that the agent should listen on for TCP based syslog messages.
          //   Defaults to '17320'.
          // port: 17320

        }

        udp: {
          // enabled - Whether or not the agent should listen for Syslog events over UDP.
          //   Defaults to 'false'.
          // enabled: false

          // address - Address that the agent should listen on for UDP based syslog messages.
          //    Defaults to ''0.0.0.0'.
          // address: "0.0.0.0"

          // msg-buf-size-bytes - The maximum allowable UDP message size.
          // msg-buf-size-bytes: 8192

          // msg-queue-size - Number of UDP syslog messages to accept and queue in agent memory. If the
          //   backlog exceeds this number the operating system will be responsible for
          //   buffering messages, and it may drop some as necessary. On Linux, for example,
          //   the maximum buffer size is determined by SO_RECVBUF. Defaults to '4096'.
          // msg-queue-size: 4096

          // port - Port that the agent should listen on for UDP based syslog messages.
          //   Defaults to '17321'.
          // port: 17321

        }
      }

      // message-formats - A list of patterns to use when parsing syslog messages. Patterns are applied to each
      // incoming message in order from first to last, until one matches. If no patterns match the agent will
      // produce an event with only the source hostname, arrival timestamp and message body. By default if this key
      // isn't specified the agent will use the RFC3164 with a year, RFC3164 and RFC5424 patterns to parse incoming
      // syslog messages.
      message-formats: [
        {
          // format - A regular expression to apply to syslog messages. If `format` matches the message
          //   attributes will be extracted and an event will be created. Note that a single backslash must be
          //   represented with two backslashes, that is, for every two backslashes entered, only a single backslash
          //   will be present in the pattern.
          //   Built-in patterns can be expanded by enclosing them in `%{}` - see the documentation for
          //   more details about regex expansion.

          //   Some named groups in this regex will override the default first-class attributes of the Event:
          //   a group called `syslog_ts` will override the `timestamp` attribute - `syslog_hostname` overrides
          //   `hostname`, and `syslog_process` overrides `service`. `location` can be overriden by a named group called
          //   `location, and `event_type_id` can be overriden by a named group called `event_type_id`.
          format: "%{SYSLOG3164YEARLINE}"

          // time-format - The format to use when parsing the `syslog_ts` group from `pattern`. The layout is specified
          //   by writing following timestamp in the expected format: Mon Jan 2 15:04:05 -0700 MST 2006. If `syslog_ts`
          //   does not match the given format, the current system time will be used instead.
          time-format: "Jan 02 15:04:05 2006"
        },
        {
          format: "%{SYSLOG3164LINE}"
          time-format: "Jan 02 15:04:05"
        },
        {
          format: "%{SYSLOG5424LINE}"
          time-format: "2006-01-02T15:04:05.000Z07:00"
        }
      ]
    }
// End sample syslog collection

/* Start sample statsd
    sample-statsd: {
      input: {

        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'.
        type: "statsd"

        udp: {
          // enabled - Whether or not the agent should listen for statsd metrics over UDP
          //   Defaults to 'false'.
          // enabled: false

          // address - Address that the agent should listen on for statsd metrics over UDP
          //   Defaults to '0.0.0.0'.
          // address: "0.0.0.0"

          // port - Port that the agent should listen on for statsd metrics over UDP
          //   Defaults to '8125'.
          // port: 8125

          // number-workers - The number of concurrent workers to read and process statsd metrics
          //   received over UDP.
          //   Defaults to '1'.
          // number-workers: 1

        }

        tcp: {
          // enabled - Whether or not the agent should listen for statsd metrics over TCP
          //   Defaults to 'false'.
          // enabled: false

          // address - Address that the agent should listen on for statsd metrics over TCP
          //   Defaults to '0.0.0.0'.
          // address: "0.0.0.0"

          // port - Port that the agent should listen on for statsd metrics over TCP
          //   Defaults to '8125'.
          // port: 8125

          // number-workers - The number of concurrent workers to read and process statsd metrics
          //   received over TCP. This number also corresponds to the maximum number of open TCP connections
          //   from statsd clients. Connections above this limit will be accepted and immediately closed.
          //   Defaults to '100'.
          // number-workers: 100
        }

      }

      // flush-interval-secs - The number of seconds between each flush of statsd metrics. All
      //   metrics received during a single flush interval are aggregated prior to sending to kafka.
      //   Defaults to '10'.
      // flush-interval-secs: 10

      // host-cache-secs - The maximum number of seconds to hold reverse DNS lookups in an application-level cache.
      //   Defaults to '300'.
      // host-cache-secs: 300

      // key-prefix - The prefix to apply to statsd metric names. For example, a statsd metric with the name 'counter1'
      //   would be sent to kafka with the name 'statsd.counter1'. The key-prefix should end with a dot. If not, a dot
      //   will be appended. A blank key-prefix is also allowed.
      //   Defaults to 'statsd.'.
      // key-prefix: "statsd."

    }
End sample statsd */

/* Start sample tailed-file
    sample-tailed-file: {
      input: {
        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'.
        type: "tailed-file"

        // path - The path to a file to be tailed. The user running the agent will need
        //   read permission on this file as well as its containing directory.
        // path: "/tmp/rocana-agent/file"

        // path-attributes-format - A regular expression containing named groupings that is used to extract
        //   named fields from the path of the file. If no value is specified, no attributes are
        //   extracted from the filename.
        // path-attributes-format: ""

        // consume-existing - Whether the agent should start reading at the beginning of a
        //   file or just start tailing. If true, on startup the agent will process
        //   the file from the top. If false, the agent will only tail for new records.
        //   If interrupted by shutdown, the agent will always pick up reading after the
        //   last line it read before shutdown, then resume tailing the file.
        //   Defaults to true
        // consume-existing: false

        // record-end-char - String that indicates the termination of a record. This string will be
        //   included with the body of the message. New line and tab chars can be specified
        //   as "\n" or "\t". Note that if 'record-end-format' is specified, it will
        //   override this configuration.
        //   Defaults to "\n".
        // record-end-char: "\n"

        // record-end-format - A regular expression that indicates the termination of a record. The string
        //   matching this pattern will be included with the body of the message.  New line
        //   and tab chars can be specified as "\n" or "\t". If this parameter is not
        //   specified, 'record-end-char' will be used instead.
        //   Using 'record-end-char' is faster and more efficient when splitting on a fixed
        //   string.
        // record-end-format: ""

      }

      batching: {
        // batch-size - The number of records to hold on to before performing a transmission to Kafka.
        //   Increasing this number will improve throughput as aproximately one RPC call to
        //   Kafka is performed per batch.  Note that increased batch size also risks an
        //   increased volume of data loss in the event of an unclean shutdown.
        //   Defaults to '10'.
        // batch-size: 10

        // timeout-ms - The number of milliseconds after the last batch flush when we should perform
        //   another flush even if we have not read 'BatchSize' number of records.
        //   Defaults to 500 milliseconds.
        // timeout-ms: 500

      }

      kafka: {
        // topic-override - The name of the Kafka topic to which Events will be written. This setting
        //   overrides the global topic for this subsection only.
        // topic-override: ""

      }

      message-attributes: {
        // format - A regular expression containing named groupings that is used to extract named
        //   fields from messages in the file.  If the overall regular expression matches
        //   the message, each named group from the regex will be used as a key/value pair
        //   in the resulting Event's attribute map.  Note that if you specify a grouping
        //   named 'location', 'host', or 'service' and there is a match, the value in the
        //   grouping will override the corresponding Event's attributes. See the
        //   description of 'time-format' for a description of how similar behavior can
        //   be achieved with timestamps.
        // format: "^(?P<timestamp>[a-zA-Z]{3} [0-9: ]+) (?P<host>[a-z.]*).*USER=(?P<location>[a-z].*) ; COMMAND=(?P<service>[/a-z \\-].*)"

        // time-format - If the regular expression matches, and there is a grouping named 'timestamp'
        //   we can use this value to set the time stamp of a message.  In order to parse
        //   the value, we need a TimeFormat that describes the layout of the timestamp.
        //   The layout is defined by specifying how the following time value would be
        //   encoded if it were in this file: Mon Jan 2 15:04:05 -0700 MST 2006. If the
        //   matched 'timestamp' value can not be parsed with this format the Event's
        //   timestamp field will not be modified but the value will still be included
        //   in the attributes mapping.  If there is no time zone or year in the time
        //   format the default of the system where the agent is running will be used.
        // time-format: "Jan 02 15:04:05"

      }

      // Data source specific default values for primary fields in Events generated
      // by this input.
      primary-field-defaults: {
        // event-type-id - The event type id for the events parsed from this file.
        //   This can be set so that events are visualized using the correct event type
        //   template in the web UI. If unspecified then the default type id for a generic
        //   tailed file record will be used. Note that event type ids of less than 1000
        //   are reserved for Rocana supplied Event types.
        // event-type-id: 105

        // host - The host on which this agent runs. If no value is specified
        //   for this setting, it will default to the hostname according to the OS.
        // host: "host02.mydomain.com"

        // location - The location name where this agent runs. Typically, this is a data center,
        //   region, or other similar designation. You are free to encode hierarchical
        //   information in this value by separating components with slashes. Defaults to
        //   an empty string.
        //   Example: nyc-1/zone-2/rack-12
        // location: ""

        // service - The name of the service producing event data. Like 'host', this will be
        //   automatically discovered in most cases. When receiving events via syslog, the
        //   service will be taken from the service field of the syslog message.
        //   Example: rocana-agent
        // service: ""

      }
    }
End sample tailed-file */

/* Start sample spool-directory
    sample-spooled-dir: {
      input: {
        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'.
        type: "spool-directory"

        // path - The path to a directory to be watched. Files copied into this directory will
        //   be read and processed. The user running the agent will need read permission
        //   on this directory and every parent directory.
        // path: "/tmp/rocana-agent/dir"

        // done-directory - The directory to move files to after they've been processed. Files which are
        //   not processed because of 'path-regex' or 'ingest-existing' will not be moved.
        //   This cannot be the same as Path. This directory must exist and be writable by
        //   the user running the agent.
        // done-directory: "/tmp/rocana-agent/dir"

        // ingest-existing - Whether the agent should pick up new files it finds in the directory when it
        //   starts. If true, on startup the agent will find and process any files created
        //   since it last shutdown. If false, these files will be ignored and the agent
        //   will immediately start watching for new files to be created. Note that the
        //   agent will always finish processing a file created before shutdown; if
        //   reading the file was interrupted by shutdown, it will continue on startup.
        //   Defaults to true
        // ingest-existing: true

        // path-attributes-format - A regular expression containing named groupings that is used to extract
        //   named fields from the path of the file. The regex is applied to the entire path.
        //   If the regex does not match any part of the path, the file will not be ingested.
        //   If no value is specified, no attributes are extracted from the filename and all files are ingested.
        // path-attributes-format: ""

        // record-end-char - String that indicates the termination of a record. This string will be
        //   included with the body of the message. New line and tab chars can be specified
        //   as "\n" or "\t". Note that if 'record-end-format' is specified, it will
        //   override this configuration.
        //   Defaults to "\n".
        // record-end-char: "\n"

        // record-end-format - A regular expression that indicates the termination of a record. The string
        //   matching this pattern will be included with the body of the message.  New line
        //   and tab chars can be specified as "\n" or "\t". If this parameter is not
        //   specified, 'record-end-char' will be used instead.
        //   Using 'record-end-char' is faster and more efficient when splitting on a fixed
        //   string.
        // record-end-format: ""

      }

      batching: {
        // batch-size - The number of records to hold on to before performing a transmission to Kafka.
        //   Increasing this number will improve throughput as aproximately one RPC call to
        //   Kafka is performed per batch.  Note that increased batch size also risks an
        //   increased volume of data loss in the event of an unclean shutdown.
        //   Defaults to '10'.
        // batch-size: 10

      }

      kafka: {
        // topic-override - The name of the Kafka topic to which Events will be written. This setting
        //   overrides the global topic for this subsection only.
        // topic-override: ""

      }

      message-attributes: {
        // format - A regular expression containing named groupings that is used to extract named
        //   fields from messages in the file.  If the overall regular expression matches
        //   the message, each named group from the regex will be used as a key/value pair
        //   in the resulting Event's attribute map.  Note that if you specify a grouping
        //   named 'location', 'host', or 'service' and there is a match, the value in the
        //   grouping will override the corresponding Event's attributes. See the
        //   description of 'time-format' for a description of how similar behavior can
        //   be achieved with timestamps.
        // format: ""

        // time-format - If the regular expression matches, and there is a grouping named 'timestamp'
        //   we can use this value to set the time stamp of a message.  In order to parse
        //   the value, we need a TimeFormat that describes the layout of the timestamp.
        //   The layout is defined by specifying how the following time value would be
        //   encoded if it were in this file: Mon Jan 2 15:04:05 -0700 MST 2006. If the
        //   matched 'timestamp' value can not be parsed with this format the Event's
        //   timestamp field will not be modified but the value will still be included
        //   in the attributes mapping.  If there is no time zone or year in the time
        //   format the default of the system where the agent is running will be used.
        // time-format: ""

      }

      // Data source specific default values for primary fields in Events generated
      // by this input.
      primary-field-defaults: {
        // event-type-id - The event type id for the events parsed from this file.
        //   This can be set so that events are visualized using the correct event type
        //   template in the web UI. If unspecified then the default type id for a generic
        //   tailed file record will be used. Note that event type ids of less than 1000
        //   are reserved for Rocana supplied Event types.
        // event-type-id: 105

        // host - The host on which this agent runs. If no value is specified
        //   for this setting, it will default to the hostname according to the OS.
        // host: "host02.mydomain.com"

        // location - The location name where this agent runs. Typically, this is a data center,
        //   region, or other similar designation. You are free to encode hierarchical
        //   information in this value by separating components with slashes. Defaults to
        //   an empty string.
        //   Example: nyc-1/zone-2/rack-12
        // location: ""

        // service - The name of the service producing event data. Like 'host', this will be
        //   automatically discovered in most cases. When receiving events via syslog, the
        //   service will be taken from the service field of the syslog message.
        //   Example: rocana-agent
        // service: ""

      }
    }
End sample spool-directory */

/* Start sample tailed-directory
    sample-tailed-directory: {
      input: {
        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'
        type: "tailed-directory"

        // directory - The path to a directory containing the files to be tailed. The user running the agent will need
        //   read permission on this directory as well as the files within.
        // directory: "/tmp/rocana-agent/"

        // filename-pattern - A regular expression to filter which files to consume from the directory. This pattern
        // is only applied to the filename and not the entire path. Required.
        // filename-pattern: ".*"

        // path-attr-pattern - A regular expression to extract attributes from the path of the file being consumed.
        // Named groups in this pattern will be added to the attribute map of events from this file. This pattern is
        // applied to the entire path of the file.
        // path-attr-pattern: ".*"

        // record-end-char - String that indicates the termination of a record. This string will be
        //   included with the body of the message. New line and tab chars can be specified
        //   as "\n" or "\t". Note that if 'record-end-format' is specified, it will
        //   override this configuration.
        //   Defaults to "\n".
        // record-end-char: "\n"

        // record-end-format - A regular expression that indicates the termination of a record. The string
        //   matching this pattern will be included with the body of the message.  New line
        //   and tab chars can be specified as "\n" or "\t". If this parameter is not
        //   specified, 'record-end-char' will be used instead.
        //   Using 'record-end-char' is faster and more efficient when splitting on a fixed
        //   string.
        // record-end-format: ""

      }

      batching: {
        // batch-size - The number of records to hold on to before performing a transmission to Kafka.
        //   Increasing this number will improve throughput as aproximately one RPC call to
        //   Kafka is performed per batch.  Note that increased batch size also risks an
        //   increased volume of data loss in the event of an unclean shutdown.
        //   Defaults to '10'.
        // batch-size: 10

      }

      kafka: {
        // topic-override - The name of the Kafka topic to which Events will be written. This setting
        //   overrides the global topic for this subsection only.
        // topic-override: ""

      }

      message-attributes: {
        // format - A regular expression containing named groupings that is used to extract named
        //   fields from messages in the file.  If the overall regular expression matches
        //   the message, each named group from the regex will be used as a key/value pair
        //   in the resulting Event's attribute map.  Note that if you specify a grouping
        //   named 'location', 'host', or 'service' and there is a match, the value in the
        //   grouping will override the corresponding Event's attributes. See the
        //   description of 'time-format' for a description of how similar behavior can
        //   be achieved with timestamps.
        // format: "^(?P<timestamp>[a-zA-Z]{3} [0-9: ]+) (?P<host>[a-z.]*).*USER=(?P<location>[a-z].*) ; COMMAND=(?P<service>[/a-z \\-].*)"

        // time-format - If the regular expression matches, and there is a grouping named 'timestamp'
        //   we can use this value to set the time stamp of a message.  In order to parse
        //   the value, we need a TimeFormat that describes the layout of the timestamp.
        //   The layout is defined by specifying how the following time value would be
        //   encoded if it were in this file: Mon Jan 2 15:04:05 -0700 MST 2006. If the
        //   matched 'timestamp' value can not be parsed with this format the Event's
        //   timestamp field will not be modified but the value will still be included
        //   in the attributes mapping.  If there is no time zone or year in the time
        //   format the default of the system where the agent is running will be used.
        // time-format: "Jan 02 15:04:05"

      }

      // Data source specific default values for primary fields in Events generated
      // by this input.
      primary-field-defaults: {
        // event-type-id - The event type id for the events parsed from this file.
        //   This can be set so that events are visualized using the correct event type
        //   template in the web UI. If unspecified then the default type id for a generic
        //   tailed file record will be used. Note that event type ids of less than 1000
        //   are reserved for Rocana supplied Event types.
        // event-type-id: 105

        // host - The host on which this agent runs. If no value is specified
        //   for this setting, it will default to the hostname according to the OS.
        // host: "host02.mydomain.com"

        // location - The location name where this agent runs. Typically, this is a data center,
        //   region, or other similar designation. You are free to encode hierarchical
        //   information in this value by separating components with slashes. Defaults to
        //   an empty string.
        //   Example: nyc-1/zone-2/rack-12
        // location: ""

        // service - The name of the service producing event data. Like 'host', this will be
        //   automatically discovered in most cases. When receiving events via syslog, the
        //   service will be taken from the service field of the syslog message.
        //   Example: rocana-agent
        // service: ""

      }

      workers: {
        // number-workers - The maximum number of files to tail concurrently. Extra filesystem events
        // will be queued until a worker is freed by waiting `yield-time-ms`. This should be at least
        // the number of log files being written to in the directory.
        // number-workers: 10

        // yield-timeout-ms - The time in milliseconds for a worker to wait for more filesystem events.
        // yield-timeout-ms: 10000

        // queue-events - The maximum number of filesystem events to hold in the queue. Beyond this number
        // events will be discarded, and the filesystem will be re-scanned when there is less load.
        // queue-events: 100
      }
    }
End sample tailed-directory */

/* Start sample pcf firehose nozzle
    sample-pcf: {
      input: {
        // type - The type of input. May be 'tailed-file', 'tailed-directory', 'spool-directory', 'statsd', 'syslog',
        //   or 'pcf'
        type: "pcf"

        // Whether or not to enable the Cloud Foundry nozzle. Set to true to connect to a Cloud Foundry environment
        // enabled: false

      }

      // api-endpoint - The CF API endpoint for the target environment you are monitoring.
      // Generally this is an HTTPS URL. For example, you only need the API endpoint URL,
      // so https://cf-api.example.com There is no default for this value; it is required
      // for token generation to allow connection to the firehose.
      // api-endpoint: "https://cf-api.example.com"

      // api-user - API user account that will be used to connect to the CF API endpoint.
      // This is a required field
      // api-user: "cf-api-user"

      // api-password - API user account's password that will be used to connect to the CF
      // API endpoint. This is a required field.
      // api-password: ""

      // api-skip-ssl-validation - Tells the Agent to ignore only CF API SSL validation
      // failures. It is recommended to leave this as false, but if the certificate is not
      // valid, then you should set this to true. Valid options are true or false.
      // api-skip-ssl-validation: false

      // api-root-ca-pem-file - Location of the PEM-encoded certificate. Specify this file
      // if you are running CF with a self-signed certificate. Should be excluded if you
      // want to use the system-wide certs.
      // api-root-ca-pem-file: "/tmp/api-ca.crt"

      // key-prefix - Metrics prefix for Rocana. This can be overridden by providing a new
      // prefix string. Affects the metric naming for PCF metrics.
      // key-prefix: "pcf."

      // firehose-traffic-controller-url - Doppler Logging Endpoint. The URL will be
      // similar to this:
      // firehose-traffic-controller-url: "wss://doppler.local.pcf:443"

      // firehose-skip-ssl-validation - Tells the Agent to ignore any Doppler endpoint SSL
      // validation failures. It is recommended to leave this as false, but if the
      // certificate is not valid, then you should set this to true. Valid options are true
      // or false.
      // firehose-skip-ssl-validation: false

      // firehose-subscription-id - CF firehose subscription ID. To load balance across
      // multiple Agents, assign the same subscription ID to all of the Agents running the
      // PCF Nozzle.
      // firehose-subscription-id: "rocana"

      // firehose-root-ca-pem-file - Location of the PEM-encoded
      // certificate. Specify this file if you are running CF with a self-signed
      // certificate. This should be excluded if you want to use the system-wide certs.
      // firehose-root-ca-pem-file: "/tmp/firehose-ca.crt"


      batching: {
        // batch-size - The number of records to hold on to before performing a transmission to Kafka.
        //   Increasing this number will improve throughput as aproximately one RPC call to
        //   Kafka is performed per batch.  Note that increased batch size also risks an
        //   increased volume of data loss in the event of an unclean shutdown.
        //   Defaults to '10'.
        // batch-size: 10

        // timeout-ms - The number of milliseconds after the last batch flush when we should perform
        //   another flush even if we have not read 'BatchSize' number of records.
        //   Defaults to 500 milliseconds.
        // timeout-ms: 500

      }

      kafka: {
        // topic-override - The name of the Kafka topic to which Events will be written. This setting
        //   overrides the global topic for this subsection only.
        // topic-override: ""

      }

      // Data source specific default values for primary fields in Events generated
      // by this input.
      primary-field-defaults: {
        // event-type-id - The event type id for the events parsed from this file.
        //   This can be set so that events are visualized using the correct event type
        //   template in the web UI. If unspecified then the default type id for a generic
        //   tailed file record will be used. Note that event type ids of less than 1000
        //   are reserved for Rocana supplied Event types.
        // event-type-id: 119

        // host - The host on which this agent runs. If no value is specified
        //   for this setting, it will default to the hostname according to the OS.
        // host: "host02.mydomain.com"

        // location - The location name where this agent runs. Typically, this is a data center,
        //   region, or other similar designation. You are free to encode hierarchical
        //   information in this value by separating components with slashes. Defaults to
        //   an empty string.
        //   Example: nyc-1/zone-2/rack-12
        // location: ""

        // service - The name of the service producing event data. Like 'host', this will be
        //   automatically discovered in most cases. When receiving events via syslog, the
        //   service will be taken from the service field of the syslog message.
        //   Example: rocana-agent
        // service: ""

      }
    }
    End sample pcf firehose nozzle */

  }
// End sources section

  host-metrics: {
    // enabled - Whether or not we should inspect this host and collect metrics about its
    //   current status. Defaults to 'true'.
    // enabled: true

    // collect-interval-secs - The frequency (in seconds) with which we should be inspecting the host and
    //   generating metrics about its current status. Defaults to '5'.
    // collect-interval-secs: 5

    kafka: {
      // topic-override - The name of the Kafka topic to which Events will be written. This setting
      //   overrides the global topic for this subsection only.
      // topic-override: ""

    }

    // Data source specific default values for primary fields in host metrics Events
    primary-field-defaults: {
      // host - The host on which this agent runs. If no value is specified
      //   for this setting, it will default to the hostname according to the OS.
      // host: "host02.mydomain.com"

      // location - The location name where this agent runs. Typically, this is a data center,
      //   region, or other similar designation. You are free to encode hierarchical
      //   information in this value by separating components with slashes. Defaults to
      //   an empty string.
      //   Example: nyc-1/zone-2/rack-12
      // location: ""

      // service - The name of the service producing event data. Like 'host', this will be
      //   automatically discovered in most cases. When receiving events via syslog, the
      //   service will be taken from the service field of the syslog message.
      //   Example: rocana-agent
      // service: ""

    }

    // publish-interval-secs - Metrics are collected about the host every 'collect-interval-secs'
    //   seconds and queued in memory.  This queue is flushed to Kafka every 'publish-interval-secs'.
    // publish-interval-secs: 60

  }

  profiler: {
    // enabled - Whether or not the diagnostic profiling endpoint is enabled. Enabled
    //   by default.
    // enabled: true

    // address - Address the HTTP service should be listening on. Defaults to '127.0.0.1'.
    // address: "127.0.0.1"

  }

  internal-metrics: {
    http-endpoint: {
      // enabled - Enabling http causes the agent to launch an http server which responds with a
      //   json struct describing key agent metrics. Defaults to true.
      // enabled: true

      // address - Address the http metrics server should listen. Defaults to '127.0.0.1'.
      // address: "127.0.0.1"

      // port - Port the http metrics server should be running on. Defaults to '17313'.
      // port: 17313

    }

    logging: {
      // enabled - Whether or not to periodically info log key metrics about the agent. Defaults to true.
      // enabled: true

      // frequency-secs - Number of seconds between metrics between each time we write metrics to the
      //   info log. Defaults to 60 seconds.
      // frequency-secs: 60

    }
  }
}
